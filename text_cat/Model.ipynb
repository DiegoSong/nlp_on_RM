{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRNNConfig(object):\n",
    "    \"\"\"RNN配置参数\"\"\"\n",
    "\n",
    "    # 模型参数\n",
    "    char_embedding_size = 100      # 词向量维度\n",
    "    char_size = 200        # 序列长度\n",
    "    num_classes = 4        # 类别数\n",
    "    vocab_size = 10000       # 词汇表达小\n",
    "\n",
    "    num_layers= 2           # 隐藏层层数\n",
    "    hidden_dim = 128        # 隐藏层神经元\n",
    "    rnn = 'lstm'             # lstm 或 gru\n",
    "\n",
    "    keep_dropout = 0.8 # dropout保留比例\n",
    "    learning_rate = 1e-3    # 学习率\n",
    "\n",
    "    batch_size = 128         # 每批训练大小\n",
    "    num_epochs = 5          # 总迭代轮次\n",
    "\n",
    "    print_per_batch = 20    # 每多少轮输出一次结果\n",
    "    save_per_batch = 10      # 每多少轮存入tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBILSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 config:TRNNConfig):\n",
    "        super(TextBILSTM, self).__init__()\n",
    "        self.num_classes = config.num_classes\n",
    "        self.learning_rate = config.learning_rate\n",
    "        self.keep_dropout = config.keep_dropout\n",
    "        self.char_embedding_size = config.char_embedding_size\n",
    "        self.l2_reg_lambda = config.l2_reg_lambda\n",
    "        self.hidden_dims = config.hidden_dims\n",
    "        self.char_size = config.char_size\n",
    "        self.rnn_layers = config.rnn_layers\n",
    "\n",
    "        self.build_model()\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        # 初始化字向量\n",
    "        self.char_embeddings = nn.Embedding(self.char_size, self.char_embedding_size)\n",
    "        # 字向量参与更新\n",
    "        self.char_embeddings.weight.requires_grad = True\n",
    "        # attention layer\n",
    "        self.attention_layer = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.attention_weights = self.attention_weights.view(self.hidden_dims, 1)\n",
    "\n",
    "        # 双层lstm\n",
    "        self.lstm_net = nn.LSTM(self.char_embedding_size, self.hidden_dims,\n",
    "                                num_layers=self.rnn_layers, dropout=self.keep_dropout,\n",
    "                                bidirectional=True)\n",
    "        # FC层\n",
    "        # self.fc_out = nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Dropout(self.keep_dropout),\n",
    "            nn.Linear(self.hidden_dims, self.hidden_dims),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.keep_dropout),\n",
    "            nn.Linear(self.hidden_dims, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def attention_net_with_w(self, lstm_out, lstm_hidden):\n",
    "        '''\n",
    "\n",
    "        :param lstm_out:    [batch_size, len_seq, n_hidden * 2]\n",
    "        :param lstm_hidden: [batch_size, num_layers * num_directions, n_hidden]\n",
    "        :return: [batch_size, n_hidden]\n",
    "        '''\n",
    "        lstm_tmp_out = torch.chunk(lstm_out, 2, -1)\n",
    "        # h [batch_size, time_step, hidden_dims]\n",
    "        h = lstm_tmp_out[0] + lstm_tmp_out[1]\n",
    "        # [batch_size, num_layers * num_directions, n_hidden]\n",
    "        lstm_hidden = torch.sum(lstm_hidden, dim=1)\n",
    "        # [batch_size, 1, n_hidden]\n",
    "        lstm_hidden = lstm_hidden.unsqueeze(1)\n",
    "        # atten_w [batch_size, 1, hidden_dims]\n",
    "        atten_w = self.attention_layer(lstm_hidden)\n",
    "        # m [batch_size, time_step, hidden_dims]\n",
    "        m = nn.Tanh()(h)\n",
    "        # atten_context [batch_size, 1, time_step]\n",
    "        atten_context = torch.bmm(atten_w, m.transpose(1, 2))\n",
    "        # softmax_w [batch_size, 1, time_step]\n",
    "        softmax_w = F.softmax(atten_context, dim=-1)\n",
    "        # context [batch_size, 1, hidden_dims]\n",
    "        context = torch.bmm(softmax_w, h)\n",
    "        result = context.squeeze(1)\n",
    "        return result\n",
    "\n",
    "    def forward(self, char_id):\n",
    "        # char_id = torch.from_numpy(np.array(input[0])).long()\n",
    "        # pinyin_id = torch.from_numpy(np.array(input[1])).long()\n",
    "\n",
    "        sen_char_input = self.char_embeddings(char_id)\n",
    "\n",
    "        # input : [len_seq, batch_size, embedding_dim]\n",
    "        sen_input = sen_char_input.permute(1, 0, 2)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm_net(sen_input)\n",
    "        # output : [batch_size, len_seq, n_hidden * 2]\n",
    "        output = output.permute(1, 0, 2)\n",
    "        # final_hidden_state : [batch_size, num_layers * num_directions, n_hidden]\n",
    "        final_hidden_state = final_hidden_state.permute(1, 0, 2)\n",
    "        # final_hidden_state = torch.mean(final_hidden_state, dim=0, keepdim=True)\n",
    "        # atten_out = self.attention_net(output, final_hidden_state)\n",
    "        atten_out = self.attention_net_with_w(output, final_hidden_state)\n",
    "        return self.fc_out(atten_out)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = joblib.load('word_to_ix.pkl')\n",
    "train = joblib.load('train.pkl')\n",
    "test = joblib.load('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
